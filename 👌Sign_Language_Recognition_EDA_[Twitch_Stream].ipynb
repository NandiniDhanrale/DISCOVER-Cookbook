{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "ðŸ‘ŒSign Language Recognition - EDA [Twitch Stream]",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NandiniDhanrale/DISCOVER-Cookbook/blob/main/%F0%9F%91%8CSign_Language_Recognition_EDA_%5BTwitch_Stream%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sign Language Recognition Challenge\n",
        "\n",
        "This notebook was created during a live coding stream. Watch it on my youtube and twitch page:\n",
        "- [Watch on Youtube](http://www.youtube.com/@robmulla?sub_confirmation=1)\n",
        "- [Live Coding on Twitch](https://www.twitch.tv/medallionstallion_)\n",
        "\n",
        "The goal of this competition is to classify isolated American Sign Language (ASL) signs.\n",
        "\n",
        "The landmarks were extracted from raw videos with the MediaPipe holistic model and are asked to predict the sign from this data."
      ],
      "metadata": {
        "id": "T0HFnqNC0yeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "plt.style.use(\"seaborn-colorblind\")"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2023-02-24T03:40:29.636971Z",
          "iopub.execute_input": "2023-02-24T03:40:29.63743Z",
          "iopub.status.idle": "2023-02-24T03:40:29.654319Z",
          "shell.execute_reply.started": "2023-02-24T03:40:29.637392Z",
          "shell.execute_reply": "2023-02-24T03:40:29.653145Z"
        },
        "trusted": true,
        "id": "EbYhpavM0yeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install nb_black for autoformatting\n",
        "!pip install nb_black --quiet\n",
        "%load_ext lab_black"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T03:26:18.826992Z",
          "iopub.execute_input": "2023-02-24T03:26:18.827442Z",
          "iopub.status.idle": "2023-02-24T03:26:30.05137Z",
          "shell.execute_reply.started": "2023-02-24T03:26:18.827405Z",
          "shell.execute_reply": "2023-02-24T03:26:30.049802Z"
        },
        "trusted": true,
        "id": "FcUQQRut0yeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data EDA"
      ],
      "metadata": {
        "id": "7itZS-zr0yeJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls ../input/asl-signs/ -GFlash --color"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T03:33:56.814042Z",
          "iopub.execute_input": "2023-02-24T03:33:56.814555Z",
          "iopub.status.idle": "2023-02-24T03:33:57.926485Z",
          "shell.execute_reply.started": "2023-02-24T03:33:56.81451Z",
          "shell.execute_reply": "2023-02-24T03:33:57.925283Z"
        },
        "trusted": true,
        "id": "P4_qIvpU0yeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '../input/asl-signs/'\n",
        "train = pd.read_csv(f'{BASE_DIR}/train.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T03:21:47.982973Z",
          "iopub.execute_input": "2023-02-24T03:21:47.983451Z",
          "iopub.status.idle": "2023-02-24T03:21:48.195027Z",
          "shell.execute_reply.started": "2023-02-24T03:21:47.983406Z",
          "shell.execute_reply": "2023-02-24T03:21:48.193939Z"
        },
        "trusted": true,
        "id": "K5LjCHWU0yeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train.csv has the path to each parquet file, the particpant id, sequence_id and sign.\n",
        "train.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T03:22:25.407389Z",
          "iopub.execute_input": "2023-02-24T03:22:25.407866Z",
          "iopub.status.idle": "2023-02-24T03:22:25.420996Z",
          "shell.execute_reply.started": "2023-02-24T03:22:25.40783Z",
          "shell.execute_reply": "2023-02-24T03:22:25.419628Z"
        },
        "trusted": true,
        "id": "v115Yi6G0yeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Signs are we trying to predict?\n",
        "- 250 Unique Signs\n",
        "- Ranging from 299-415 Examples of Each"
      ],
      "metadata": {
        "id": "TwH7xgu40yeK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "train[\"sign\"].value_counts().head(50).sort_values(ascending=True).plot(\n",
        "    kind=\"barh\", ax=ax, title=\"Top 50 Signs in Training Dataset\"\n",
        ")\n",
        "ax.set_xlabel(\"Number of Training Examples\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T03:28:55.623932Z",
          "iopub.execute_input": "2023-02-24T03:28:55.624398Z",
          "iopub.status.idle": "2023-02-24T03:28:56.381254Z",
          "shell.execute_reply.started": "2023-02-24T03:28:55.624358Z",
          "shell.execute_reply": "2023-02-24T03:28:56.380027Z"
        },
        "trusted": true,
        "id": "rbApje2U0yeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "train[\"sign\"].value_counts().tail(50).sort_values(ascending=True).plot(\n",
        "    kind=\"barh\", ax=ax, title=\"Bottom 50 Signs in Training Dataset\"\n",
        ")\n",
        "ax.set_xlabel(\"Number of Training Examples\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T03:29:05.913704Z",
          "iopub.execute_input": "2023-02-24T03:29:05.914417Z",
          "iopub.status.idle": "2023-02-24T03:29:06.680829Z",
          "shell.execute_reply.started": "2023-02-24T03:29:05.914372Z",
          "shell.execute_reply": "2023-02-24T03:29:06.679375Z"
        },
        "trusted": true,
        "id": "V_9GxZkw0yeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parquet Landmark Data\n",
        "- Each Parquet file is in the path:\n",
        "    - train_landmark_files/[participant_id]/[sequence_id].parquet\n",
        "- The parquet's associated sign can be found in train.csv"
      ],
      "metadata": {
        "id": "PPSqz2ul0yeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pull an example parquet file data...\n",
        "\n",
        "We pull an example landmark file for the sign \"listen\""
      ],
      "metadata": {
        "id": "H-oi8-bJ0yeL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_fn = train.query('sign == \"listen\"')[\"path\"].values[0]\n",
        "\n",
        "example_landmark = pd.read_parquet(f\"{BASE_DIR}/{example_fn}\")\n",
        "example_landmark.head()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T03:35:40.18355Z",
          "iopub.execute_input": "2023-02-24T03:35:40.184773Z",
          "iopub.status.idle": "2023-02-24T03:35:40.22041Z",
          "shell.execute_reply.started": "2023-02-24T03:35:40.184692Z",
          "shell.execute_reply": "2023-02-24T03:35:40.219146Z"
        },
        "trusted": true,
        "id": "0fy1ECDS0yeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_frames = example_landmark[\"frame\"].nunique()\n",
        "unique_types = example_landmark[\"type\"].nunique()\n",
        "types_in_video = example_landmark[\"type\"].unique()\n",
        "print(\n",
        "    f\"The file has {unique_frames} unique frames and {unique_types} unique types: {types_in_video}\"\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T03:38:07.78651Z",
          "iopub.execute_input": "2023-02-24T03:38:07.78699Z",
          "iopub.status.idle": "2023-02-24T03:38:07.804937Z",
          "shell.execute_reply.started": "2023-02-24T03:38:07.786955Z",
          "shell.execute_reply": "2023-02-24T03:38:07.803707Z"
        },
        "trusted": true,
        "id": "_VM7aVbV0yeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lets Compare for a bunch of parquet files what type of data we have.\n",
        "- We notice the number of frames is not consistent\n",
        "- Almost every file has 4 types of landmarks: face, left_hand, pose and right_hand."
      ],
      "metadata": {
        "id": "ytf7J1180yeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "listen_files = train.query('sign == \"listen\"')[\"path\"].values\n",
        "for i, f in enumerate(listen_files):\n",
        "    example_landmark = pd.read_parquet(f\"{BASE_DIR}/{f}\")\n",
        "    unique_frames = example_landmark[\"frame\"].nunique()\n",
        "    unique_types = example_landmark[\"type\"].nunique()\n",
        "    types_in_video = example_landmark[\"type\"].unique()\n",
        "    print(\n",
        "        f\"The file has {unique_frames} unique frames and {unique_types} unique types: {types_in_video}\"\n",
        "    )\n",
        "    if i == 20:\n",
        "        break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T03:39:29.128683Z",
          "iopub.execute_input": "2023-02-24T03:39:29.129804Z",
          "iopub.status.idle": "2023-02-24T03:39:29.8649Z",
          "shell.execute_reply.started": "2023-02-24T03:39:29.129754Z",
          "shell.execute_reply": "2023-02-24T03:39:29.86367Z"
        },
        "trusted": true,
        "id": "dthN5HUu0yeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create Metadata for Training Dataset"
      ],
      "metadata": {
        "id": "p4Tdl9mB0yeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N_PARQUETS_TO_READ = 100_000  # So we don't have to load all 95k\n",
        "\n",
        "combined_meta = {}\n",
        "for i, d in tqdm(train.iterrows(), total=len(train)):\n",
        "    file_path = d[\"path\"]\n",
        "    example_landmark = pd.read_parquet(f\"{BASE_DIR}/{file_path}\")\n",
        "    # Get the number of landmarks with x,y,z data per type\n",
        "    meta = (\n",
        "        example_landmark.dropna(subset=[\"x\", \"y\", \"z\"])[\"type\"].value_counts().to_dict()\n",
        "    )\n",
        "    meta[\"frames\"] = example_landmark[\"frame\"].nunique()\n",
        "    xyz_meta = (\n",
        "        example_landmark.agg(\n",
        "            {\n",
        "                \"x\": [\"min\", \"max\", \"mean\"],\n",
        "                \"y\": [\"min\", \"max\", \"mean\"],\n",
        "                \"z\": [\"min\", \"max\", \"mean\"],\n",
        "            }\n",
        "        )\n",
        "        .unstack()\n",
        "        .to_dict()\n",
        "    )\n",
        "\n",
        "    for key in xyz_meta.keys():\n",
        "        new_key = key[0] + \"_\" + key[1]\n",
        "        meta[new_key] = xyz_meta[key]\n",
        "    combined_meta[file_path] = meta\n",
        "    if i >= N_PARQUETS_TO_READ:\n",
        "        break"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:08:09.255486Z",
          "iopub.execute_input": "2023-02-24T04:08:09.256116Z",
          "iopub.status.idle": "2023-02-24T04:08:34.356146Z",
          "shell.execute_reply.started": "2023-02-24T04:08:09.256063Z",
          "shell.execute_reply": "2023-02-24T04:08:34.354692Z"
        },
        "trusted": true,
        "id": "rO7e18xr0yeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_with_meta = train.merge(\n",
        "    pd.DataFrame(combined_meta).T.reset_index().rename(columns={\"index\": \"path\"}),\n",
        "    how=\"left\",\n",
        ")\n",
        "train_with_meta.to_parquet(\"train_with_meta.parquet\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:20:14.805526Z",
          "iopub.execute_input": "2023-02-24T04:20:14.806005Z",
          "iopub.status.idle": "2023-02-24T04:20:15.023399Z",
          "shell.execute_reply.started": "2023-02-24T04:20:14.805967Z",
          "shell.execute_reply": "2023-02-24T04:20:15.022194Z"
        },
        "trusted": true,
        "id": "2WB2_NFG0yeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What are the most frequent types of landmarks provided?\n",
        "- Face has a lot more datapoints because mediapipe provides 468 3D datapoints per frame."
      ],
      "metadata": {
        "id": "n0WFkmlJ0yeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_with_meta[[\"face\", \"pose\", \"left_hand\", \"right_hand\"]].sum().sort_values().plot(\n",
        "    kind=\"barh\", title=\"Sum of Rows by Landmark Type\"\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:08:37.82348Z",
          "iopub.execute_input": "2023-02-24T04:08:37.824762Z",
          "iopub.status.idle": "2023-02-24T04:08:38.060036Z",
          "shell.execute_reply.started": "2023-02-24T04:08:37.82471Z",
          "shell.execute_reply": "2023-02-24T04:08:38.058613Z"
        },
        "trusted": true,
        "id": "Cx0F0ZX_0yeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every parquet file has at least some datapoints for all four types of landmarks:\n",
        "    - Face, pose, left hand and right hand."
      ],
      "metadata": {
        "id": "urOHBcqN0yeN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking to see if the number of landmarks for this type is zero\n",
        "(\n",
        "    train_with_meta.query(\"index < 1000\").fillna(0)[\n",
        "        [\"face\", \"pose\", \"left_hand\", \"right_hand\"]\n",
        "    ]\n",
        "    > 0\n",
        ").mean().plot(kind=\"barh\", title=\"Rate of Frame/Keypoints with Data\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:10:59.88481Z",
          "iopub.execute_input": "2023-02-24T04:10:59.885263Z",
          "iopub.status.idle": "2023-02-24T04:11:00.116649Z",
          "shell.execute_reply.started": "2023-02-24T04:10:59.885219Z",
          "shell.execute_reply": "2023-02-24T04:11:00.115644Z"
        },
        "trusted": true,
        "id": "tHWAGqAt0yeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check out one Example"
      ],
      "metadata": {
        "id": "bxnHAzCp0yeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_fn = train_with_meta.dropna().query('sign == \"shhh\"')[\"path\"].values[0]\n",
        "example_landmark = pd.read_parquet(f\"{BASE_DIR}/{example_fn}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:16:55.252641Z",
          "iopub.execute_input": "2023-02-24T04:16:55.253668Z",
          "iopub.status.idle": "2023-02-24T04:16:55.298008Z",
          "shell.execute_reply.started": "2023-02-24T04:16:55.253625Z",
          "shell.execute_reply": "2023-02-24T04:16:55.296888Z"
        },
        "trusted": true,
        "id": "--YZNiE60yeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_landmark.query(\"frame == 25\")[\"type\"].value_counts()  # Middle of the video"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:16:55.940319Z",
          "iopub.execute_input": "2023-02-24T04:16:55.941162Z",
          "iopub.status.idle": "2023-02-24T04:16:55.956671Z",
          "shell.execute_reply.started": "2023-02-24T04:16:55.941119Z",
          "shell.execute_reply": "2023-02-24T04:16:55.955339Z"
        },
        "trusted": true,
        "id": "nR3CNoAX0yeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_landmark[\"no_xyz\"] = example_landmark[\"x\"].isna()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:17:24.810954Z",
          "iopub.execute_input": "2023-02-24T04:17:24.811384Z",
          "iopub.status.idle": "2023-02-24T04:17:24.822663Z",
          "shell.execute_reply.started": "2023-02-24T04:17:24.811351Z",
          "shell.execute_reply": "2023-02-24T04:17:24.82144Z"
        },
        "trusted": true,
        "id": "iywTX0jm0yeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_landmark.groupby(\"frame\")[\"no_xyz\"].sum().plot(\n",
        "    title=\"missing xyz per frame\", kind=\"bar\"\n",
        ")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:18:00.382343Z",
          "iopub.execute_input": "2023-02-24T04:18:00.382855Z",
          "iopub.status.idle": "2023-02-24T04:18:00.765982Z",
          "shell.execute_reply.started": "2023-02-24T04:18:00.382812Z",
          "shell.execute_reply": "2023-02-24T04:18:00.764487Z"
        },
        "trusted": true,
        "id": "x6v5H9r00yeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3D plot of Landmarks from \"shhh\" example\n",
        "Pick frame 17 because we have no missing xyz data"
      ],
      "metadata": {
        "id": "1tk69xE-0yeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "example_frame = example_landmark.query(\"frame == 17\")\n",
        "px.scatter_3d(example_frame, x=\"x\", y=\"y\", z=\"z\", color=\"type\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:18:30.274901Z",
          "iopub.execute_input": "2023-02-24T04:18:30.275398Z",
          "iopub.status.idle": "2023-02-24T04:18:30.360158Z",
          "shell.execute_reply.started": "2023-02-24T04:18:30.275358Z",
          "shell.execute_reply": "2023-02-24T04:18:30.358886Z"
        },
        "trusted": true,
        "id": "ECcdPmT80yeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_landmark[\"y_\"] = example_landmark[\"y\"] * -1\n",
        "example_frame = example_landmark.query(\"frame == 17 and type== 'face'\")\n",
        "px.scatter(example_frame, x=\"x\", y=\"y_\", color=\"type\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:49:45.825401Z",
          "iopub.execute_input": "2023-02-24T04:49:45.825904Z",
          "iopub.status.idle": "2023-02-24T04:49:45.893903Z",
          "shell.execute_reply.started": "2023-02-24T04:49:45.82586Z",
          "shell.execute_reply": "2023-02-24T04:49:45.892832Z"
        },
        "trusted": true,
        "id": "NaIDfmHG0yeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try to draw the example with mediapipe's hand connections?"
      ],
      "metadata": {
        "id": "_DH7N-J20yeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mediapipe --quiet"
      ],
      "metadata": {
        "id": "g_GVYwyR0yeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mediapipe as mp\n",
        "\n",
        "mp_hands = mp.solutions.hands\n",
        "\n",
        "\n",
        "example_landmark[\"y_\"] = example_landmark[\"y\"] * -1\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 5))\n",
        "\n",
        "for hand in [\"left_hand\", \"right_hand\"]:\n",
        "    example_hand = example_landmark.query(\"frame == 17 and type == @hand\")\n",
        "\n",
        "    ax.scatter(example_hand[\"x\"], example_hand[\"y_\"])\n",
        "\n",
        "    for connection in mp_hands.HAND_CONNECTIONS:\n",
        "        point_a = connection[0]\n",
        "        point_b = connection[1]\n",
        "        x1, y1 = example_hand.query(\"landmark_index == @point_a\")[[\"x\", \"y_\"]].values[0]\n",
        "        x2, y2 = example_hand.query(\"landmark_index == @point_b\")[[\"x\", \"y_\"]].values[0]\n",
        "        plt.plot([x1, x2], [y1, y2], color=\"purple\")\n",
        "ax.set_title(\"Shhh - Hands Data\")\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T05:07:12.358132Z",
          "iopub.execute_input": "2023-02-24T05:07:12.358924Z",
          "iopub.status.idle": "2023-02-24T05:07:12.910189Z",
          "shell.execute_reply.started": "2023-02-24T05:07:12.358879Z",
          "shell.execute_reply": "2023-02-24T05:07:12.909266Z"
        },
        "trusted": true,
        "id": "1YXJcJ5h0yeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try to use mediapipe to plot\n",
        "- Pull some example images\n",
        "- Run mediapipe holistic to see how it produces the results\n",
        "- plot them on the image"
      ],
      "metadata": {
        "id": "aTQFUmbr0yeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://i.ytimg.com/vi/mi9f9zOaqM8/hqdefault.jpg --quiet\n",
        "!wget https://previews.123rf.com/images/mimagephotography/mimagephotography1411/mimagephotography141100022/33214722-full-length-portrait-of-a-fashionable-young-man-standing-on-isolated-white-background.jpg --quiet"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:46:49.835716Z",
          "iopub.execute_input": "2023-02-24T04:46:49.836149Z",
          "iopub.status.idle": "2023-02-24T04:46:53.964011Z",
          "shell.execute_reply.started": "2023-02-24T04:46:49.836115Z",
          "shell.execute_reply": "2023-02-24T04:46:53.962602Z"
        },
        "trusted": true,
        "id": "97tVImPq0yeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import mediapipe as mp\n",
        "\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "mp_drawing_styles = mp.solutions.drawing_styles\n",
        "mp_holistic = mp.solutions.holistic\n",
        "\n",
        "# For static images:\n",
        "IMAGE_FILES = [\n",
        "    \"hqdefault.jpg\",\n",
        "    \"33214722-full-length-portrait-of-a-fashionable-young-man-standing-on-isolated-white-background.jpg\",\n",
        "]\n",
        "BG_COLOR = (192, 192, 192)  # gray\n",
        "with mp_holistic.Holistic(\n",
        "    static_image_mode=True,\n",
        "    model_complexity=2,\n",
        "    enable_segmentation=True,\n",
        "    refine_face_landmarks=True,\n",
        ") as holistic:\n",
        "    for idx, file in enumerate(IMAGE_FILES):\n",
        "        image = cv2.imread(file)\n",
        "        image_height, image_width, _ = image.shape\n",
        "        # Convert the BGR image to RGB before processing.\n",
        "        results = holistic.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "        if results.pose_landmarks:\n",
        "            print(\n",
        "                f\"Nose coordinates: (\"\n",
        "                f\"{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].x * image_width}, \"\n",
        "                f\"{results.pose_landmarks.landmark[mp_holistic.PoseLandmark.NOSE].y * image_height})\"\n",
        "            )\n",
        "\n",
        "        annotated_image = image.copy()\n",
        "        # Draw segmentation on the image.\n",
        "        # To improve segmentation around boundaries, consider applying a joint\n",
        "        # bilateral filter to \"results.segmentation_mask\" with \"image\".\n",
        "        condition = np.stack((results.segmentation_mask,) * 3, axis=-1) > 0.1\n",
        "        bg_image = np.zeros(image.shape, dtype=np.uint8)\n",
        "        bg_image[:] = BG_COLOR\n",
        "        annotated_image = np.where(condition, annotated_image, bg_image)\n",
        "        # Draw pose, left and right hands, and face landmarks on the image.\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            results.face_landmarks,\n",
        "            mp_holistic.FACEMESH_TESSELATION,\n",
        "            landmark_drawing_spec=None,\n",
        "            connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style(),\n",
        "        )\n",
        "        mp_drawing.draw_landmarks(\n",
        "            annotated_image,\n",
        "            results.pose_landmarks,\n",
        "            mp_holistic.POSE_CONNECTIONS,\n",
        "            landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style(),\n",
        "        )\n",
        "        cv2.imwrite(\"/tmp/annotated_image\" + str(idx) + \".png\", annotated_image)\n",
        "        # Plot pose world landmarks.\n",
        "#         mp_drawing.plot_landmarks(\n",
        "#             results.pose_world_landmarks, mp_holistic.POSE_CONNECTIONS\n",
        "#         )"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:36:15.545884Z",
          "iopub.execute_input": "2023-02-24T04:36:15.546352Z",
          "iopub.status.idle": "2023-02-24T04:36:16.557752Z",
          "shell.execute_reply.started": "2023-02-24T04:36:15.546311Z",
          "shell.execute_reply": "2023-02-24T04:36:16.55641Z"
        },
        "trusted": true,
        "id": "K7VIGJC60yeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(plt.imread(\"/tmp/annotated_image\" + str(0) + \".png\"))\n",
        "plt.show()\n",
        "\n",
        "plt.imshow(plt.imread(\"/tmp/annotated_image\" + str(1) + \".png\"))\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:36:23.657902Z",
          "iopub.execute_input": "2023-02-24T04:36:23.658373Z",
          "iopub.status.idle": "2023-02-24T04:36:24.358504Z",
          "shell.execute_reply.started": "2023-02-24T04:36:23.658333Z",
          "shell.execute_reply": "2023-02-24T04:36:24.35751Z"
        },
        "trusted": true,
        "id": "ttYSyNLD0yeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try to use the same format for plotting of parquet data"
      ],
      "metadata": {
        "id": "zIiXfAIW0yeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "background_image = np.zeros([720, 720, 3])\n",
        "\n",
        "mp_drawing.draw_landmarks(\n",
        "    background_image,\n",
        "    results.face_landmarks,\n",
        "    mp_holistic.FACEMESH_TESSELATION,\n",
        "    landmark_drawing_spec=None,\n",
        "    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style(),\n",
        ")\n",
        "mp_drawing.draw_landmarks(\n",
        "    background_image,\n",
        "    results.pose_landmarks,\n",
        "    mp_holistic.POSE_CONNECTIONS,\n",
        "    landmark_drawing_spec=mp_drawing_styles.get_default_pose_landmarks_style(),\n",
        ")\n",
        "plt.imshow(background_image)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:45:19.099361Z",
          "iopub.execute_input": "2023-02-24T04:45:19.099845Z",
          "iopub.status.idle": "2023-02-24T04:45:19.535578Z",
          "shell.execute_reply.started": "2023-02-24T04:45:19.099805Z",
          "shell.execute_reply": "2023-02-24T04:45:19.534685Z"
        },
        "trusted": true,
        "id": "PkkE02w10yeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(results.face_landmarks)\n",
        "\n",
        "from mediapipe.framework.formats import landmark_pb2\n",
        "\n",
        "# face_landmarks = landmark_pb2.NormalizedLandmarkList(example_frame.query('type == \"face\"')[[\"x\", \"y\", \"z\"]].values)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T04:43:44.220496Z",
          "iopub.execute_input": "2023-02-24T04:43:44.221102Z",
          "iopub.status.idle": "2023-02-24T04:43:44.229017Z",
          "shell.execute_reply.started": "2023-02-24T04:43:44.221053Z",
          "shell.execute_reply": "2023-02-24T04:43:44.227819Z"
        },
        "trusted": true,
        "id": "8mGf-I9H0yeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TODO\n",
        "- Figure out how to transform the parquet file data into mediapipe `NormalizedLandmarkList`"
      ],
      "metadata": {
        "id": "tlrdjOM30yeW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "The evaluation metric for this contest is simple classification accuracy.\n",
        "\n",
        "Below code was taken from the evaluation page."
      ],
      "metadata": {
        "id": "iW9HsyUf0yeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_relevant_data_subset(pq_path):\n",
        "    data_columns = ['x', 'y', 'z']\n",
        "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
        "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
        "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
        "    return data.astype(np.float32)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-02-24T03:18:37.333391Z",
          "iopub.execute_input": "2023-02-24T03:18:37.334426Z",
          "iopub.status.idle": "2023-02-24T03:18:37.341853Z",
          "shell.execute_reply.started": "2023-02-24T03:18:37.334374Z",
          "shell.execute_reply": "2023-02-24T03:18:37.340506Z"
        },
        "trusted": true,
        "id": "5mS8PkfB0yeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tflite_runtime.interpreter as tflite\n",
        "\n",
        "# def run_model(model_path):\n",
        "#     interpreter = tflite.Interpreter(model_path)\n",
        "\n",
        "#     found_signatures = list(interpreter.get_signature_list().keys())\n",
        "\n",
        "#     if REQUIRED_SIGNATURE not in found_signatures:\n",
        "#         raise KernelEvalException('Required input signature not found.')\n",
        "\n",
        "#     prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
        "#     output = prediction_fn(inputs=frames)\n",
        "#     sign = np.argmax(output[\"outputs\"])"
      ],
      "metadata": {
        "id": "VY6M3heU0yeW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}